# 使用多卡进行训练

在flag参数中对  ps_tasks改为1，num_clones改为2，可以将batch_size进行更改为2

或者在训练代码前加上指定GPU序号，如下：

```python
CUDA_VISIBLE_DEVICES = 0,1,2 python<训练代码>
```



## tensorflow分布式训练

参数含义：

- ps：作为分布式训练的服务端，等到各个终端(supervisors)来连接。
- worker：在Tensorflow的代码注释中被称为supervisors，作为分布式的终端。
- chief supervisors：在众多终端中必须选中一个作为主要的运算终端。该终端是在运算终端中最先启动，它的功能是合并各个终端运算后的参数，并保存写入。

分工：

- 服务端作为协调者，等待各个运算终端来连接
- chief supervisors会在启动时统一管理全局的学习参数，进行初始化或从模型载入
- 其它终端只负责运算，不保存相应的数据（例如：checkpoint和tensorboard需要的日志）。

参考地址：https://www.cnblogs.com/zyly/p/8880894.html

